{
 "metadata": {
  "name": "",
  "signature": "sha256:35037822d2b76f2c68a1f38c79e35a438964cdf033a3203fa31bdb488fdb86c9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model dynamics:\n",
      "* Consider an option price which evolves according to \n",
      "$$ \\text{d} S_{t} = \\mu S_{t} \\text{d}t + \\sigma S_{t} \\text{d} W_{t} $$ \n",
      "* We use an Euler discretization of the form $$ S_{t+h} = S_{t} \\text{exp} \\left [ \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h + \\sigma \\sqrt{h} Z_{t} \\right ], $$ \n",
      "for a small $h$, where $Z_{t} \\sim \\text{N}(0,1)$ and are independent.\n",
      "$$ \\Rightarrow \\text{log}(S_{t+h}) \\sim \\mathcal{N} \\left( \\text{log}(S_{t}) + \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h, \\sigma^{2} h \\right )  $$\n",
      "* We consider an Asian Option of the form $ \\sum_{i=1}^{m} f(S_{t_{i}}) $ where $0 < t_{1} < \\ldots < t_{m} = T $ is a set of montoring dates and $f$ is some function. \n",
      "* We want to estimate the price of this option, that is, we want to estimate \n",
      "$$ \\mathbb{E} \\left [ \\sum_{i=1}^{m} f(S_{t_{i}}) \\right ] = \\int \\left [ \\sum_{i=1}^{m} f(s_{t_{i}}) \\right ] p(s_{1:T}|s_{0}) \\text{d} s_{1:T} $$\n",
      "where $s_{0}$ is the price at time $0$.\n",
      "* For the moment, we assume that $t_{1} = k, t_{2} = 2k, \\ldots, t_{m} = mk = N $, that is, we simulate forward one day at a time and look at the price after every $k$ days. \n",
      "* Instead of the expectation above, let us (for the time being) target $ \\text{E} | \\sum_{i=1}^{m} f(S_{t_{i}}) | $\n",
      "* Henceforth, we change our notation to let $S$ denote the log-price, in which case the distributions become normal instead of lognormal.\n",
      "\n",
      "\n",
      "### Idea: \n",
      "(Everything is conditioned on $s_{0}$)\n",
      "* The general way of doing SIR would be to first sample from the sequence of densities which are proportional to $$ p(s_{1}), p(s_{1:2}), \\ldots, p(s_{1:(k-1)}), |f(s_{k})|^{\\kappa_{1}} p(s_{1:k}) $$\n",
      "$$ p(s_{1:(k+1)}), p(s_{1:(k+2)}), \\ldots, p(s_{1:(2k-1)}), |\\sum_{i=1}^{2} |f(s_{ik})|^{\\kappa_{2}} p(s_{1:2k}) $$\n",
      "$$ \\vdots $$\n",
      "$$ p(s_{1:(m-1)k+1}), \\ldots, p(s_{1:mk-1}), |\\sum_{i=1}^{m} f(s_{ik})| ^{\\kappa_{m}} p(s_{1:mk}), $$\n",
      "where $ 0 \\leq \\kappa_{1} < \\ldots < \\kappa_{m} < 1 $ and the process densities are used as proposals.\n",
      "* Given these samples, we use SMC samplers to sample from the sequence of densities\n",
      "$$ \\pi_{n}^{\\prime}(s_{1:N}) \\propto | \\sum_{i=1}^{m} f(s_{ik}) |^{\\tilde{\\kappa}_{n}} p(s_{1:N}) $$\n",
      "for $n = 1, 2, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
      "* The SIR algorithm is pretty straightforward to apply, but the SMC sampler is more complicated. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calling libraries:\n",
      "\n",
      "%matplotlib inline  \n",
      "from __future__ import division\n",
      "import numpy as np\n",
      "import time\n",
      "from scipy.stats import norm, uniform\n",
      "from pylab import plot, show, legend\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.figsize'] = (16.0, 5.0) #for larger plots\n",
      "\n",
      "def f(x):\n",
      "    return x\n",
      "np.vectorize(f)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "<numpy.lib.function_base.vectorize at 0x36e41d0>"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SIR: \n",
      "\n",
      "* Since our model is quite simple, what we can do is that we sample from the sequence of densities \n",
      "$$ \\pi_{n}(s_{t_{1}},\\ldots,s_{t_{n}}) \\propto |\\sum_{i=1}^{n} f(s_{t_{i}}) |^{\\kappa_{n}} p(s_{t_{1}},\\ldots,s_{t_{n}}) = |\\sum_{i=1}^{n} f(s_{ik}) |^{\\kappa_{n}} p(s_{k},\\ldots,s_{nk}) $$\n",
      "for $n = 1, \\ldots, m$. \n",
      "* We can do this because we know the process densities of $S_{k}, S_{2k}, \\ldots, S_{mk}$ and we can simulate $S_{0} \\rightarrow S_{k} \\rightarrow S_{2k} \\rightarrow \\cdots \\rightarrow S_{mk}$ directly without any intermediate points of the path. \n",
      "* Henceforth, we refer to these as $S_{1:m}$ for notational convenience.\n",
      "\n",
      "What are the incremental weights in this case?\n",
      "* We observe that we are targeting $\\gamma_{n}(s_{1:n}) = |\\sum_{i=1}^{n}(s_{i})|^{\\kappa_{n}} p(s_{1:n}) $ at the $n$-th time step. \n",
      "* Our proposals are $q_{n}(s_{1:n}) =$ process density of $S_{1:n}$.\n",
      "* In this case, the weights are (for $n = 1,2, \\ldots, m$): \n",
      "$$ w_{n}(s_{1:n}) = \\frac{ \\gamma_{n}(s_{1:n}) }{ q_{n}(s_{1:n}) } = \\frac{ \\gamma_{n-1}(s_{1:n-1}) }{ q_{n-1}(s_{1:n-1}) } \\times \\frac{ \\gamma_{n}(s_{1:n})}{ \\gamma_{n-1}(s_{1:n-1}) q_{n}(s_{n}|s_{1:n-1})} = w_{n-1}(s_{1:n-1}) \\alpha_{n}(s_{1:n}) $$\n",
      "where \n",
      "$$ \\alpha_{n}(s_{1:n}) = \\frac{ \\gamma_{n} (s_{1:n}) } { \\gamma_{n-1} (s_{1:n-1}) q_{n}(s_{n}|s_{1:n-1}) } = \\frac{ | \\sum_{i=1}^{n} f(s_{i}) |^{\\kappa_{n}} p(s_{1:n}) } { | \\sum_{i=1}^{n-1} f(s_{i}) |^{\\kappa_{n-1}} p(s_{1:n-1})} \\frac{1}{p(s_{n}|s_{1:n-1}) } = \\frac{ | \\sum_{i=1}^{n} f(s_{i}) |^{\\kappa_{n}} } { | \\sum_{i=1}^{n-1} f(s_{i}) |^{\\kappa_{n-1}} } $$\n",
      "where $p(\\cdot)$ denotes the process density of the Brownian motion.\n",
      "\n",
      "A note on the normalizing constant: \n",
      "* $Z_{1}^{SIR} = \\int p(s_{1}) \\text{d} s_{1} = 1$.\n",
      "* $Z_{m}^{SIR} = \\int |\\sum_{i=1}^{m} f(s_{i})|^{\\kappa_{m}} p(s_{1:m}) \\text{d} s_{1:m} $ is the normalizing constant in the SIR algorithm, which we estimate. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SIR algorithm: \n",
      "\n",
      "def SIR(m,k,S_0,sigma,mu,M,c):\n",
      "    \n",
      "    S = np.zeros((M,m+1))\n",
      "    S[:,0], weights = [S_0]*M, [1/M]*M\n",
      "    cut_off, normalizing_constant = M*c, 1.0\n",
      "    mean, sd = (mu-0.5*sigma**2)*k/360, sigma*np.sqrt(k/360)\n",
      "    kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
      "    \n",
      "    S[:,1] = S[:,0] + mean + sd*norm.rvs(0,1,M)\n",
      "    for n in np.add(2,range(m-1)):                                          # n = 2, 3, ..., m\n",
      "        weights /= np.power(np.absolute(np.sum(f(S[:,1:n]),axis=1)),kappa[n-2])\n",
      "        S[:,n] = S[:,n-1] + mean + sd*norm.rvs(0,1,M)\n",
      "        weights *= np.power(np.absolute(np.sum(f(S[:,1:(n+1)]),axis=1)),kappa[n-1])\n",
      "        normalizing_constant *= np.sum(weights) \n",
      "        weights /= np.sum(weights)\n",
      "        ESS = 1/np.sum(weights**2)\n",
      "        if(ESS<cut_off):\n",
      "            S[:,1:(n+1)] = S[np.random.choice(a=range(M),size=M,p=weights),1:(n+1)]\n",
      "            weights = [1/M]*M \n",
      "        \n",
      "    return S[:,1:(m+1)], weights, normalizing_constant"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Sanity check: (is SIR code correct?)\n",
      "* The normalizing constant in SIR is approximating $ \\text{E} |\\sum_{i=1}^{m} f(S_{i}) |^{\\kappa_{m}} $\n",
      "* Let us do a naive Monte Carlo to verify that this is working properly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Naive Monte Carlo to check if SIR is working okay:\n",
      "\n",
      "def naive_MC_SIR(m,k,S_0,sigma,mu,M):\n",
      "    S, f_S = [S_0]*M, np.zeros(M) \n",
      "    mean, sd = (mu-0.5*sigma**2)*k/360, sigma*np.sqrt(k/360)\n",
      "    for i in range(m):                                                   # i = 1, 2, ..., m\n",
      "        S += mean + sd*norm.rvs(0,1,M)\n",
      "        f_S += f(S)\n",
      "    return np.mean(np.power(np.absolute(f_S),kappa[m-1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, k, M = 30, 30, 10**4\n",
      "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
      "S_0, sigma, mu = np.log(100), 3.0, 0.0\n",
      "c_SIR = 0.5\n",
      "\n",
      "\n",
      "SIR(m,k,S_0,sigma,mu,M,c_SIR)[2], naive_MC_SIR(m,k,S_0,sigma,mu,M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "(7.8642482038376063, 7.8967446828813843)"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, k, M = 15, 60, 10**4\n",
      "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
      "S_0, sigma, mu = np.log(100), 2.0, 0.0\n",
      "c_SIR = 0.5\n",
      "\n",
      "SIR(m,k,S_0,sigma,mu,M,c_SIR)[2], naive_MC_SIR(m,k,S_0,sigma,mu,M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "(5.3921299544327983, 5.3788348835426429)"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Okay, so SIR seems to be working okay! \n",
      "* This took some time to get correct (unfortunately), but at least it works. \n",
      "* Is the variance of the SIR estimate less than the variance of the naive MC estimate?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SMC Sampler:\n",
      "* Again, we do a simplificaton because the model is simple. \n",
      "* What we want to do is to sample from \n",
      "$$ \\tilde{\\pi}_{n} (s_{1:m}) \\propto |\\sum_{i=1}^{m} f(s_{i}) |^{\\tilde{\\kappa}_{n}} p(s_{1:m}) $$\n",
      "for $n = 1, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
      "* SIR gives a bunch of particles $\\{ s_{1:m}^{(l)} \\}_{l=1}^{M}$ with weights $\\{ w^{(l)} \\}_{l=1}^{M} $.\n",
      "* Let $x^{(l)} = \\{ s_{1:m}^{(l)} \\}$, $l = 1, \\ldots, M$.\n",
      "\n",
      "So how do we do SMC sampling? \n",
      "* For each particle $x^{(l)}$, we initialize particles $x_{0}^{(l)} = x^{(l)}$ and weights $w_{0}^{(l)} = w^{(l)}$. We do this using the SIR algorithm algorithm. The weights are obtained from the SIR algorithm.\n",
      "* Move $x_{n-1}^{(l)} \\rightarrow x_{n}^{(l)}$ by using kernel $K_{n}(x_{n-1}^{(l)},x_{n})$.\n",
      "* Compute incremental weights:\n",
      "$$ W_{n}^{(l)}(x_{(n-1):n}^{(l)}) = \\frac{ \\tilde{\\pi}_{n}(x_{n}^{(l)}) L_{n-1}(x_{n}^{(l)},x_{n-1}^{(l)})}{ \\tilde{\\pi}_{n-1}(x_{n-1}^{(l)}) K_{n}(x_{n-1}^{(l)},x_{n}^{(l)}) } $$ \n",
      "and then resample if need be.\n",
      "* The choice of backward kernel is:\n",
      "$$ L_{n-1}(x_{n},x_{n-1}) = \\frac{ \\tilde{\\pi}_{n}(x_{n-1}) K_{n}(x_{n-1},x_{n}) }{ \\tilde{\\pi}_{n}(x_{n}) }, $$\n",
      "in which case the incremental weights reduce down to:\n",
      "$$ W_{n}^{(l)}(x_{n-1}^{(l)}) = \\frac{\\tilde{\\pi}_{n}(x_{n-1}^{(l)})}{\\tilde{\\pi}_{n-1}(x_{n-1}^{(l)})} = \\frac{ | \\sum_{i=1}^{m} f(s_{i,n-1}^{(l)}) |^{\\tilde{\\kappa}_{n}} } { | \\sum_{i=1}^{m} f(s_{i,n-1}^{(l)}) |^{\\tilde{\\kappa}_{n-1}} }, \\text{ } n \\geq 1, $$\n",
      "where $ x_{n}^{(l)} = s_{1:m,n}^{(l)}$, $n = 1, 2, \\ldots, p$ and $l = 1, 2, \\ldots, M$.\n",
      "\n",
      "* What is $ \\tilde{\\pi}_{0}(x_{0}^{(l)})$? These are (from the SIR model): \n",
      "$$\\pi_{m}(s^{(l)}_{1:m}) \\propto | \\sum_{i=1}^{m} f(s^{(l)}_{i}) |^{\\kappa_{m}} p(s^{(l)}_{1:m}) = \\tilde{\\pi}_{n} (s^{(l)}_{1:m})$$ \n",
      "as $\\tilde{\\kappa}_{n} = \\kappa_{m} $.\n",
      "\n",
      "The question that remains is how to choose the kernel $K_{n}$?\n",
      "* We want $K_{n}(x_{n-1},\\cdot)$ to be $\\tilde{\\pi}_{n}(\\cdot)$ invariant.\n",
      "* The simplest way to do this is to do a symmetric Metropolis-Hastings random walk.\n",
      "* Therefore, propose $S_{n} \\sim \\text{N}(x_{n-1},\\text{I})$ and accept with probability \n",
      "$$1 \\wedge \\frac{\\tilde{\\pi}_{n}(X_{n})}{\\tilde{\\pi}_{n}(s_{n-1})} = 1 \\wedge \\frac{|\\sum_{i=1}^{m}f(S_{i})|^{\\tilde{\\kappa}_{n}} p(S_{1:m})}{ |\\sum_{i=1}^{m}f(s_{i})|^{\\tilde{\\kappa}_{n}} p(s_{1:m}) } $$\n",
      "\n",
      "A note on the normalizing constant:\n",
      "* $Z_{1}^{SMC} = \\int | \\sum_{i=1}^{m} f(s_{i}) |^{\\tilde{\\kappa}_{1}} p(s_{1:m}) \\text{d} s_{1:m} = Z_{m}^{SIR}$ as $\\tilde{\\kappa}_{1} = \\kappa_{m}$.\n",
      "* $ Z_{p}^{SMC} = \\int | \\sum_{i=1}^{m} f(s_{i}) | p(s_{1:m}) \\text{d} s_{1:m} $ is the normalizing constant in the SMC algorithm, and is precisely what we want to estimate."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SMC samplers: \n",
      "\n",
      "# We work on a log scale for densities because they tend to be tiny.\n",
      "\n",
      "# p(s_{1:m}): \n",
      "# This is the density of the path of a Brownian Motion starting at S_0\n",
      "# We consider the log of the density\n",
      "def log_path_density(sigma,mu,s_0,path):\n",
      "    path = np.append(s_0,path)\n",
      "    N = np.size(path)\n",
      "    mean = path[0:(N-1)] + (mu-0.5*sigma**2)*k/360.0\n",
      "    sd = sigma*np.sqrt(k/360.0)\n",
      "    return np.sum(np.log(norm.pdf(path[1:N],loc=mean,scale=sd)))\n",
      "np.vectorize(log_path_density)\n",
      "\n",
      "\n",
      "\"\"\" For the moment we don't actually use the following function, but these may be useful later.\n",
      "\n",
      "# pi_{n}(s_{1:m})\n",
      "# This is the density that we are targeting at time n:\n",
      "def SMC_log_target_density(sigma,mu,kap,s_0,path):\n",
      "    return np.log(np.power(np.abs(np.sum(f(path))),kap)) + log_path_density(sigma,mu,s_0,path)\n",
      "np.vectorize(SMC_log_target_density)\n",
      "\n",
      "\"\"\"\n",
      "\n",
      "# Symmetric random walk Metropolis-Hastings kernel:\n",
      "# (No new array created)\n",
      "def propagate(original,proposed,n):\n",
      "    proposed = norm.rvs(loc=original,scale=1.0) \n",
      "    a = np.power(np.absolute(np.sum(f(proposed))/np.sum(f(original))),kappa_tilde[n])\n",
      "    b = np.exp(log_path_density(sigma,mu,S_0,proposed)-log_path_density(sigma,mu,S_0,original))\n",
      "    MH_ratio = a*b\n",
      "    acceptance_probability = 1.0\n",
      "    if(MH_ratio<1.0):\n",
      "        acceptance_probability = MH_ratio\n",
      "    u = uniform.rvs(size=1)\n",
      "    if(u<acceptance_probability):\n",
      "        original = proposed\n",
      "    return original"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Defining the SMC function:\n",
      "\n",
      "def SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde):\n",
      "    \n",
      "    m = np.int(m)\n",
      "    \n",
      "    # We first run SIR with M particles:\n",
      "    S, weights, SIR_normalizing_constant = SIR(m,k,S_0,sigma,mu,M,c_SIR,kappa)\n",
      "    \n",
      "    # Next define some necessary things:\n",
      "    cut_off, normalizing_constant = c_SMC*M, SIR_normalizing_constant\n",
      "    incremental_weights = np.zeros(M)\n",
      "    p = np.size(kappa_tilde)-1\n",
      "    proposed = np.zeros(M)                                               \n",
      "    # (this is a dummy variable defined so that we don't need to define a new array everytime we propagate rows of S)\n",
      "    \n",
      "    for n in np.add(1,range(p)):                                              # n = 1, 2, ..., p\n",
      "        \n",
      "        # Propagating and computing incremental weights:\n",
      "        # (hopefully we shall later be able to do without the loop over i)\n",
      "        # incremental_weights = np.power(np.absolute(np.sum(f(S),axis=1)),kappa_tilde[n]-kappa_tilde[n-1])\n",
      "        # (the above line calculates the incremental weights, but how to propagate without the loop?)\n",
      "        for i in range(M):                                                              \n",
      "            incremental_weights[i] = np.power(np.absolute(np.sum(f(S[i,:]))),kappa_tilde[n]-kappa_tilde[n-1])\n",
      "            S[i,:] = propagate(S[i,:],proposed,n)\n",
      "        \n",
      "        weights *= incremental_weights\n",
      "        normalizing_constant *= np.sum(weights) \n",
      "        weights /= np.sum(weights)\n",
      "        ESS = 1/np.sum(weights**2)\n",
      "        if(ESS<cut_off):\n",
      "            S = S[np.random.choice(a=range(M),size=M,p=weights),:]\n",
      "            weights = [1/M]*M \n",
      "        \n",
      "    return normalizing_constant "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Naive Monte Carlo: \n",
      "* As a comparison, we can also use naive Monte Carlo to estimate the price. \n",
      "* We simulate a bunch of $M$ paths of the process independently of each other and then estimate the option price as \n",
      "$$ \\frac{1}{M} \\sum_{l=1}^{M} \\left [ \\sum_{i=1}^{m} f(S_{i}^{(l)}) \\right ] $$\n",
      "where we again use the simplification $S_{1:m}$ for $S_{t_{1}:t_{m}}$ nd where $S_{1:m}^{(l)}$ denotes the $l$-th path.\n",
      "* However, since the SMC algorithm is essentially approximating $ \\text{E} | \\sum_{i=1}^{m} f(S_{i}) | $, let us also use naive Monte Carlo to estimate the same quantity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Naive Monte Carlo\n",
      "\n",
      "def naive_MC(m,k,S_0,sigma,mu,M):\n",
      "    m = np.int(m)\n",
      "    price = np.zeros(M)\n",
      "    S = [S_0]*M \n",
      "    mean = [(mu - 0.5*sigma**2)*k/360.0]*M\n",
      "    sd = sigma*np.sqrt(k/360.0)\n",
      "    for i in np.add(1,range(m)):                                                    # i = 1, 2, ..., m\n",
      "        S = np.add(S,mean+sd*norm.rvs(0,1,M))\n",
      "        price += f(S)\n",
      "    return np.mean(np.absolute(price)) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##### Another sanity check (is the code for the SMC sampler correct?):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, k, M, p = 20.0, 30, 10**4, 15\n",
      "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
      "kappa_tilde = np.linspace(start=0.5,stop=1.0,num=p)\n",
      "S_0, sigma, mu = np.log(100), 3.0, 0.0\n",
      "c_SIR, c_SMC = 0.5, 0.5\n",
      "\n",
      "print \"kappa =\", kappa\n",
      "print \"kappa_tilde =\", kappa_tilde\n",
      "\n",
      "SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde), naive_MC(m,k,S_0,sigma,mu,M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "kappa = [ 0.          0.02631579  0.05263158  0.07894737  0.10526316  0.13157895\n",
        "  0.15789474  0.18421053  0.21052632  0.23684211  0.26315789  0.28947368\n",
        "  0.31578947  0.34210526  0.36842105  0.39473684  0.42105263  0.44736842\n",
        "  0.47368421  0.5       ]\n",
        "kappa_tilde = [ 0.5         0.53571429  0.57142857  0.60714286  0.64285714  0.67857143\n",
        "  0.71428571  0.75        0.78571429  0.82142857  0.85714286  0.89285714\n",
        "  0.92857143  0.96428571  1.        ]\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "(38.345143447700892, 38.673511212198612)"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, k, M, p = 20.0, 30, 10**3, 15\n",
      "m = np.int(m)\n",
      "\n",
      "survival_probability_naive_MC = np.zeros(m)\n",
      "survival_probability_SMC = np.zeros(m)\n",
      "naive_MC_price = np.zeros(m)\n",
      "SMC_sampler_price = np.zeros(m)\n",
      "\n",
      "kappa_tilde = np.linspace(start=0.5,stop=1.0,num=p)\n",
      "\n",
      "for t in np.add(1,range(m)):                              # t = 1, 2, ..., m\n",
      "        \n",
      "    kappa = np.linspace(start=0.0,stop=0.5,num=t)\n",
      "        \n",
      "    naive_MC_price[t-1] = naive_MC(t,k,S_0,sigma,mu,M)\n",
      "    SMC_sampler_price[t-1] = SMC(t,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde)\n",
      "    \n",
      "plt.figure()\n",
      "plt.plot(np.add(1,range(m)),naive_MC_price,\"r*\",np.add(1,range(m)),SMC_sampler_price,\"b-\")\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAA6AAAAE4CAYAAABFb562AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0wAAAABJRU5ErkJggg==\n",
       "text": [
        "<matplotlib.figure.Figure at 0x9fc24e0>"
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* The code is working. \n",
      "* I was an imbecile. My code for the naive Monte Carlo was incorrect, but the SMC code was fine."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Basic comparison study: \n",
      "* We now look at the standard errors for the naive MC estimate and the SMC estimate. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rep = 50\n",
      "SMC_estimate = np.zeros(rep)\n",
      "MC_estimate = np.zeros(rep)\n",
      "\n",
      "overall_start_time = time.clock()\n",
      "\n",
      "start_time = time.clock()\n",
      "for r in range(rep):\n",
      "    SMC_estimate[r] = SMC(m,k,S_0,sigma,mu,10**3,c_SIR,c_SMC,kappa,kappa_tilde)\n",
      "average_SMC_run_time = (time.clock() - start_time)/rep\n",
      "\n",
      "start_time = time.clock()\n",
      "for r in range(rep):\n",
      "    MC_estimate[r] = naive_MC(m,k,S_0,sigma,mu,10**4) \n",
      "average_MC_run_time = (time.clock() - start_time)/rep\n",
      "\n",
      "overall_run_time = time.clock() - overall_start_time\n",
      "\n",
      "print \"SMC estimate =\", np.mean(SMC_estimate)\n",
      "print \"MC estimate =\", np.mean(MC_estimate)\n",
      "print \"SMC standard deviation =\", np.std(SMC_estimate)\n",
      "print \"MC standard deviation =\", np.std(MC_estimate)\n",
      "print \"SMC average run time =\", average_SMC_run_time, \"seconds\"\n",
      "print \"MC average run time =\", average_MC_run_time, \"seconds\"\n",
      "print \"Overall run time for \", rep, \"repetitions =\", overall_run_time/60, \"minutes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "SMC estimate = 38.7543834101\n",
        "MC estimate = 38.5976005452\n",
        "SMC standard deviation = 0.725420055595\n",
        "MC standard deviation = 0.254918027862\n",
        "SMC average run time = 4.96248263856 seconds\n",
        "MC average run time = 0.0101401468498 seconds\n",
        "Overall run time for  50 repetitions = 4.14385494631 minutes\n"
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Overall run time for \", rep, \"repetitions =\", overall_run_time/60, \"minutes\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " Overall run time for  50 repetitions = 46.7764310687 minutes\n"
       ]
      }
     ],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}