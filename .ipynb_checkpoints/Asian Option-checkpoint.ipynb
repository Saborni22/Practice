{
 "metadata": {
  "name": "",
  "signature": "sha256:5d36a8736ef1f84bb029c14eb693aef4e13ab73dba7d770eb3cd8e509e1c3216"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Model dynamics:\n",
      "* Consider an option price which evolves according to \n",
      "$$ \\text{d} S_{t} = \\mu S_{t} \\text{d}t + \\sigma S_{t} \\text{d} W_{t} $$ \n",
      "* We use an Euler discretization of the form $$ S_{t+h} = S_{t} \\text{exp} \\left [ \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h + \\sigma \\sqrt{h} Z_{t} \\right ], $$ \n",
      "for a small $h$, where $Z_{t} \\sim \\text{N}(0,1)$ and are independent.\n",
      "$$ \\Rightarrow \\text{log}(S_{t+h}) \\sim \\text{N} \\left( \\text{log}(S_{t}) + \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h, \\sigma^{2} h \\right )  $$\n",
      "* We consider an Asian Option of the form $ \\sum_{i=1}^{m} f(S_{t_{i}}) $ where $0 < t_{1} < \\ldots < t_{m} = T $ is a set of montoring dates and $f$ is some function. \n",
      "* We want to estimate the price of this option, that is, we want to estimate \n",
      "$$ \\text{E} \\left [ \\sum_{i=1}^{m} f(S_{t_{i}}) \\right ] = \\int \\left [ \\sum_{i=1}^{m} f(s_{t_{i}}) \\right ] p(s_{1:T}|s_{0}) \\text{d} s_{1:T} $$\n",
      "where $s_{0}$ is the price at time $0$.\n",
      "* For the moment, we assume that $t_{1} = k, t_{2} = 2k, \\ldots, t_{m} = mk = N $, that is, we simulate forward one day at a time and look at the price after every $k$ days. \n",
      "* Henceforth, we change our notation to let $S$ denote the log-price, in which case the distributions become normal instead of lognormal.\n",
      "\n",
      "### Idea: \n",
      "(Everything is conditioned on $s_{0}$)\n",
      "* The general way of doing SIR would be to first sample from the sequence of densities which are proportional to $$ p(s_{1}), p(s_{1:2}), \\ldots, p(s_{1:(k-1)}), |f(s_{k})|^{\\kappa_{1}} p(s_{1:k}) $$\n",
      "$$ p(s_{1:(k+1)}), p(s_{1:(k+2)}), \\ldots, p(s_{1:(2k-1)}), |\\sum_{i=1}^{2} |f(s_{ik})|^{\\kappa_{2}} p(s_{1:2k}) $$\n",
      "$$ \\vdots $$\n",
      "$$ p(s_{1:(m-1)k+1}), \\ldots, p(s_{1:mk-1}), |\\sum_{i=1}^{m} f(s_{ik}| ^{\\kappa_{m}} p(s_{1:mk}), $$\n",
      "where $ 0 \\leq \\kappa_{1} < \\ldots < \\kappa_{m} < 1 $ and the process densities are used as proposals.\n",
      "* Given these samples, we use SMC samplers to sample from the sequence of densities\n",
      "$$ \\pi_{n}(s_{1:N}) \\propto | \\sum_{i=1}^{m} f(s_{ik}) |^{\\tilde{\\kappa}_{n}} p(s_{1:N}) $$\n",
      "for $n = 1, 2, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
      "* The SIR algorithm is pretty straightforward to apply, but the SMC sampler is more complicated. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Calling libraries:\n",
      "\n",
      "%matplotlib inline  \n",
      "from __future__ import division\n",
      "import numpy as np\n",
      "import time\n",
      "from scipy.stats import norm, uniform\n",
      "from pylab import plot, show, legend\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.figsize'] = (16.0, 5.0) #for larger plots\n",
      "\n",
      "def f(x):\n",
      "    return x\n",
      "np.vectorize(f) "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 71,
       "text": [
        "<numpy.lib.function_base.vectorize at 0x1f649b0>"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SIR: \n",
      "\n",
      "* Since our model is quite simple, what we can do is that we sample from the sequence of densities \n",
      "$$ \\pi_{n}(s_{t_{1}},\\ldots,s_{t_{n}}) \\propto |\\sum_{i=1}^{n} f(s_{t_{i}}) |^{\\kappa_{n}} p(s_{t_{1}},\\ldots,s_{t_{n}}) = |\\sum_{i=1}^{n} f(s_{ik}) |^{\\kappa_{n}} p(s_{k},\\ldots,s_{nk}) $$\n",
      "for $n = 1, \\ldots, m$. \n",
      "* We can do this because we know the process densities of $S_{k}, S_{2k}, \\ldots, S_{mk}$. These are Gaussian (conditioned on $S_{0}$).\n",
      "* Henceforth, we refer to these as $S_{1:m}$ for notational convenience."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SIR algorithm: \n",
      "\n",
      "def SIR(m,k,S_0,sigma,mu,M,c,kappa):\n",
      "    S = np.zeros((M,m+1))\n",
      "    S[:,0] = [S_0]*M\n",
      "    weights = [1/M]*M\n",
      "    cut_off = M*c\n",
      "    normalizing_constant = 1\n",
      "    for i in np.add(1,range(m)):                                                   # i = 1, 2, ..., m\n",
      "        # Propagating and calculating weights and normalizing constant:\n",
      "        S[:,i] = S[:,i-1] + (mu - 0.5*sigma**2)*k/360 + sigma*np.sqrt(k/360)*norm.rvs(0,1,M)\n",
      "        incremental_weights = np.power(np.absolute(np.sum(f(S[:,0:(i+1)]),axis=1)),kappa[i-1])\n",
      "        normalizing_constant = normalizing_constant*np.sum(incremental_weights) \n",
      "        weights = weights*incremental_weights\n",
      "        # Adaptive resampling:\n",
      "        weights /= np.sum(weights)\n",
      "        ESS = 1/np.sum(weights**2)\n",
      "        if(ESS<cut_off):\n",
      "            S[:,0:(i+1)] = S[np.random.choice(a=range(M),size=M,p=weights),0:(i+1)] \n",
      "            weights = [1/M]*M \n",
      "    return S[:,1:(m+1)], weights, normalizing_constant"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### SMC Sampler:\n",
      "* Again, we do a simplificaton because the model is simple. \n",
      "* What we want to do is to sample from \n",
      "$$ \\pi_{n} (s_{1:m}) \\propto |\\sum_{i=1}^{m} f(s_{i}) |^{\\tilde{\\kappa}_{n}} p(s_{1:m}) $$\n",
      "for $n = 1, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
      "* SIR gives a bunch of particles $\\{ s_{1:m}^{(l)} \\}_{l=1}^{M}$.\n",
      "* Let $x^{(l)} = \\{ s_{1:m}^{(l)} \\}$, $l = 1, \\ldots, M$.\n",
      "\n",
      "So how do we do SMC sampling? \n",
      "* For each particle $x^{(l)}$, we initialize particles $x_{0}^{(l)} = x^{(l)}$ and weights $w_{0}^{(l)} = 1/M $. Why are weights $1/M$ ?? \n",
      "* Move $x_{n-1}^{(l)} \\rightarrow x_{n}^{(l)}$ by using kernel $K(x_{n-1}^{(l)},x_{n})$.\n",
      "* Compute incremental weights \n",
      "$$ W_{n}^{(l)}(x_{(n-1):n}^{(l)}) = \\frac{ \\pi_{n}(x_{n}^{(l)}) L_{n-1}(x_{n}^{(l)},x_{n-1}^{(l)})}{ \\pi_{n-1}(x_{n-1}^{(l)}) K_{n}(x_{n-1}^{(l)},x_{n}^{(l)}) } $$ \n",
      "and then resample if need be.\n",
      "* The choice of backward kernel is \n",
      "$$ L_{n-1}(x_{n},x_{n-1}) = \\frac{ \\pi_{n}(x_{n-1}) K_{n}(x_{n-1},x_{n}) }{ \\pi_{n}(x_{n}) } $$\n",
      "* Then the incremental weights reduce to \n",
      "$$ W_{n}^{(l)}(x_{(n-1):n}^{(l)}) = \\frac{\\pi_{n}(x_{n-1}^{(l)})}{\\pi_{n-1}(x_{n-1}^{(l)})} $$\n",
      "\n",
      "The question that remains is how to choose the kernel $K_{n}$?\n",
      "* We want $K_{n}(x_{n-1},\\cdot)$ to be $\\pi_{n}(\\cdot)$ invariant.\n",
      "* The simplest way to do this is to do a symmetric Metropolis-Hastings random walk.\n",
      "* Therefore, propose $X_{n} \\sim \\text{N}(x_{n-1},\\text{I})$ and accept with probability \n",
      "$$1 \\wedge \\frac{\\pi_{n}(X_{n})}{\\pi_{n}(x_{n-1})} $$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SMC samplers: \n",
      "# We work on a log scale for densities because they tend to be tiny.\n",
      "# We suppose that the initial particles are all equally weighted.\n",
      "\n",
      "# p(s_{1:m}): \n",
      "# This is the density of the path of a Brownian Motion\n",
      "def log_path_density(sigma,mu,s_0,path):\n",
      "    path = np.append(s_0,path)\n",
      "    N = np.size(path)\n",
      "    mean = path[0:N-1] + (mu-0.5*sigma**2)*np.divide(np.add(1,range(N-1)),360.0)\n",
      "    var = sigma*np.sqrt(np.divide(np.add(1,range(N-1)),360.0))\n",
      "    return np.sum(np.log(norm.pdf(path[1:N],loc=mean,scale=var)))\n",
      "np.vectorize(log_path_density)\n",
      "\n",
      "# pi_{n}(s_{1:m})\n",
      "# This is the density that we are targeting at time n:\n",
      "def SMC_log_target_density(sigma,mu,kap,s_0,path):\n",
      "    return np.log(np.power(np.abs(np.sum(f(path))),kap)) + log_path_density(sigma,mu,s_0,path)\n",
      "np.vectorize(SMC_log_target_density)\n",
      "\n",
      "# Symmetric random walk Metropolis-Hastings kernel:\n",
      "# (No new array created)\n",
      "def propagate(original,proposed):\n",
      "    N = np.size(original)\n",
      "    proposed = norm.rvs(loc=original,scale=1) \n",
      "    MH_ratio = np.exp(log_path_density(sigma,mu,S_0,proposed)-log_path_density(sigma,mu,S_0,original))\n",
      "    acceptance_probability = 1.0\n",
      "    if(MH_ratio<=1.0):\n",
      "        acceptance_probability = MH_ratio\n",
      "    u = uniform.rvs(loc=0.0,scale=1.0,size=1)\n",
      "    if(u<=acceptance_probability):\n",
      "        original = proposed\n",
      "\n",
      "def SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde):\n",
      "    m = np.int(m)\n",
      "    \n",
      "    # We first run SIR and then resample the particles to ensure they have equal weights\n",
      "    run_SIR = SIR(m,k,S_0,sigma,mu,M,c_SIR,kappa)\n",
      "    S, weights, SIR_normalizing_constant = run_SIR[0], run_SIR[1], run_SIR[2]\n",
      "    S = S[np.random.choice(a=range(M),size=M,p=weights),:]\n",
      "    \n",
      "    # Next we define some necessary things\n",
      "    cut_off = c_SMC*M\n",
      "    normalizing_constant = 1.0\n",
      "    incremental_weights = np.zeros(M)\n",
      "    p = np.size(kappa_tilde)\n",
      "    proposed = np.zeros(M)                                               \n",
      "    # (this is a dummy variable defined so that we don't need to define a new array everytime we propagate rows of S)\n",
      "    \n",
      "    # Then we do SMC sampling:\n",
      "    for n in range(p):\n",
      "        \n",
      "        # Propagating and computing incremental weights:\n",
      "        # (hopefully we shall later be able to do without the loop over i)\n",
      "        for i in range(M):                                                              \n",
      "            incremental_weights[i] = np.exp((SMC_log_target_density(sigma,mu,kappa_tilde[n],S_0,S[i,:])-SMC_log_target_density(sigma,mu,kappa_tilde[n-1],S_0,S[i,:])))\n",
      "            propagate(S[i,:],proposed)\n",
      "        \n",
      "        # Normalizing constant:\n",
      "        normalizing_constant = normalizing_constant*np.sum(incremental_weights)\n",
      "        \n",
      "        # Adaptive resampling:\n",
      "        weights = weights*incremental_weights\n",
      "        weights /= np.sum(weights)\n",
      "        ESS = 1/np.sum(weights**2)\n",
      "        if(ESS<cut_off):\n",
      "            S = S[np.random.choice(a=range(M),size=M,p=weights),:]\n",
      "            weights = [1/M]*M \n",
      "        \n",
      "    return normalizing_constant "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m, k, M = 10.0, 30, 10**3\n",
      "kappa = np.divide(range(np.int(2*m+1)),2*m)[0:np.int(m)]\n",
      "kappa_tilde = np.divide(range(np.int(2*m+1)),2*m)[np.int(m-1):np.int(2*m+1)]\n",
      "S_0, sigma, mu = np.log(100), 3.0, 0.0\n",
      "c_SIR, c_SMC = 0.5, 0.5"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 74
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "8.6212864580871952e+35"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Naive Monte Carlo: \n",
      "* As a comparison, we can also use naive Monte Carlo to estimate the price. \n",
      "* We simulate a bunch of $M$ paths of the process independently of each other and then estimate the option price as \n",
      "$$ \\frac{1}{M} \\sum_{l=1}^{M} \\left [ \\sum_{i=1}^{m} f(S_{i}^{(l)}) \\right ] $$\n",
      "where we again use the simplification $S_{1:m}$ for $S_{t_{1}:t_{m}}$ nd where $S_{1:m}^{(l)}$ denotes the $l$-th path.\n",
      "* However, since the SMC algorithm is essentially approximating $ \\text{E} | \\sum_{i=1}^{m} f(S_{i}) | $, let us also use naive Monte Carlo to estimate the same quantity. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Naive Monte Carlo\n",
      "\n",
      "def naive_MC(m,k,S_0,sigma,mu,M):\n",
      "    m = np.int(m)\n",
      "    S = np.zeros((M,m+1))\n",
      "    S[:,0] = [S_0]*M \n",
      "    price = 0.0\n",
      "    for i in np.add(1,range(m)):                                                    # i = 1, 2, ..., m\n",
      "        S[:,i] = S[:,i-1] + (mu - 0.5*sigma**2)*k/360 + sigma*np.sqrt(k/360)*norm.rvs(0,1,M)\n",
      "    price = np.sum(np.abs(f(S[:,1:(m+1)])))/M\n",
      "    return price"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "naive_MC(m,k,S_0,sigma,mu,M)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 79,
       "text": [
        "29.775337414823159"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}