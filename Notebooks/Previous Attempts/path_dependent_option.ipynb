{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model dynamics:\n",
    "* Consider an option price which evolves according to \n",
    "$$ \\text{d} S_{t} = \\mu S_{t} \\text{d}t + \\sigma S_{t} \\text{d} W_{t} $$ \n",
    "* We use an Euler discretization of the form $$ S_{t+h} = S_{t} \\text{exp} \\left [ \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h + \\sigma \\sqrt{h} Z_{t} \\right ], $$ \n",
    "for a small $h$, where $Z_{t} \\sim \\text{N}(0,1)$ and are independent.\n",
    "$$ \\Rightarrow \\text{log}(S_{t+h}) \\sim \\mathcal{N} \\left( \\text{log}(S_{t}) + \\left( \\mu - \\frac{1}{2} \\sigma^{2} \\right ) h, \\sigma^{2} h \\right )  $$\n",
    "* We consider an Asian Option of the form $ \\sum_{i=1}^{m} f(S_{t_{i}}) $ where $0 < t_{1} < \\ldots < t_{m} = T $ is a set of montoring dates and $f$ is some function. \n",
    "* We want to estimate the price of this option, that is, we want to estimate \n",
    "$$ \\mathbb{E} \\left [ \\sum_{i=1}^{m} f(S_{t_{i}}) \\right ] = \\int \\left [ \\sum_{i=1}^{m} f(s_{t_{i}}) \\right ] p(s_{1:T}|s_{0}) \\text{d} s_{1:T} $$\n",
    "where $s_{0}$ is the price at time $0$.\n",
    "* For the moment, we assume that $t_{1} = k, t_{2} = 2k, \\ldots, t_{m} = mk = N $, that is, we simulate forward one day at a time and look at the price after every $k$ days. \n",
    "* Instead of the expectation above, let us (for the time being) target $ \\text{E} | \\sum_{i=1}^{m} f(S_{t_{i}}) | $\n",
    "* Henceforth, we change our notation to let $S$ denote the log-price, in which case the distributions become normal instead of lognormal.\n",
    "\n",
    "\n",
    "### Idea: \n",
    "(Everything is conditioned on $s_{0}$)\n",
    "* The general way of doing SIR would be to first sample from the sequence of densities which are proportional to $$ p(s_{1}), p(s_{1:2}), \\ldots, p(s_{1:(k-1)}), |f(s_{k})|^{\\kappa_{1}} p(s_{1:k}) $$\n",
    "$$ p(s_{1:(k+1)}), p(s_{1:(k+2)}), \\ldots, p(s_{1:(2k-1)}), |\\sum_{i=1}^{2} |f(s_{ik})|^{\\kappa_{2}} p(s_{1:2k}) $$\n",
    "$$ \\vdots $$\n",
    "$$ p(s_{1:(m-1)k+1}), \\ldots, p(s_{1:mk-1}), |\\sum_{i=1}^{m} f(s_{ik})| ^{\\kappa_{m}} p(s_{1:mk}), $$\n",
    "where $ 0 \\leq \\kappa_{1} < \\ldots < \\kappa_{m} < 1 $ and the process densities are used as proposals.\n",
    "* Given these samples, we use SMC samplers to sample from the sequence of densities\n",
    "$$ \\pi_{n}^{\\prime}(s_{1:N}) \\propto | \\sum_{i=1}^{m} f(s_{ik}) |^{\\tilde{\\kappa}_{n}} p(s_{1:N}) $$\n",
    "for $n = 1, 2, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
    "* The SIR algorithm is pretty straightforward to apply, but the SMC sampler is more complicated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.function_base.vectorize at 0x395f4e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling libraries:\n",
    "\n",
    "%matplotlib inline  \n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import time\n",
    "from scipy.stats import norm, uniform\n",
    "from pylab import plot, show, legend\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (16.0, 5.0) #for larger plots\n",
    "\n",
    "def f(x):\n",
    "    return x\n",
    "np.vectorize(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SIR: \n",
    "\n",
    "* Since our model is quite simple, what we can do is that we sample from the sequence of densities \n",
    "$$ \\pi_{n}(s_{t_{1}},\\ldots,s_{t_{n}}) \\propto |\\sum_{i=1}^{n} f(s_{t_{i}}) |^{\\kappa_{n}} p(s_{t_{1}},\\ldots,s_{t_{n}}) = |\\sum_{i=1}^{n} f(s_{ik}) |^{\\kappa_{n}} p(s_{k},\\ldots,s_{nk}) $$\n",
    "for $n = 1, \\ldots, m$. \n",
    "* We can do this because we know the process densities of $S_{k}, S_{2k}, \\ldots, S_{mk}$ and we can simulate $S_{0} \\rightarrow S_{k} \\rightarrow S_{2k} \\rightarrow \\cdots \\rightarrow S_{mk}$ directly without any intermediate points of the path. \n",
    "* Henceforth, we refer to these as $S_{1:m}$ for notational convenience.\n",
    "\n",
    "What are the incremental weights in this case?\n",
    "* We observe that we are targeting $\\gamma_{n}(s_{1:n}) = |\\sum_{i=1}^{n}(s_{i})|^{\\kappa_{n}} p(s_{1:n}) $ at the $n$-th time step. \n",
    "* Our proposals are $q_{n}(s_{1:n}) =$ process density of $S_{1:n}$.\n",
    "* In this case, the weights are (for $n = 1,2, \\ldots, m$): \n",
    "$$ w_{n}(s_{1:n}) = \\frac{ \\gamma_{n}(s_{1:n}) }{ q_{n}(s_{1:n}) } = \\frac{ \\gamma_{n-1}(s_{1:n-1}) }{ q_{n-1}(s_{1:n-1}) } \\times \\frac{ \\gamma_{n}(s_{1:n})}{ \\gamma_{n-1}(s_{1:n-1}) q_{n}(s_{n}|s_{1:n-1})} = w_{n-1}(s_{1:n-1}) \\alpha_{n}(s_{1:n}) $$\n",
    "where \n",
    "$$ \\alpha_{n}(s_{1:n}) = \\frac{ \\gamma_{n} (s_{1:n}) } { \\gamma_{n-1} (s_{1:n-1}) q_{n}(s_{n}|s_{1:n-1}) } = \\frac{ | \\sum_{i=1}^{n} f(s_{i}) |^{\\kappa_{n}} p(s_{1:n}) } { | \\sum_{i=1}^{n-1} f(s_{i}) |^{\\kappa_{n-1}} p(s_{1:n-1})} \\frac{1}{p(s_{n}|s_{1:n-1}) } = \\frac{ | \\sum_{i=1}^{n} f(s_{i}) |^{\\kappa_{n}} } { | \\sum_{i=1}^{n-1} f(s_{i}) |^{\\kappa_{n-1}} } $$\n",
    "where $p(\\cdot)$ denotes the process density of the Brownian motion.\n",
    "\n",
    "A note on the normalizing constant: \n",
    "* $Z_{1}^{SIR} = \\int p(s_{1}) \\text{d} s_{1} = 1$.\n",
    "* $Z_{m}^{SIR} = \\int |\\sum_{i=1}^{m} f(s_{i})|^{\\kappa_{m}} p(s_{1:m}) \\text{d} s_{1:m} $ is the normalizing constant in the SIR algorithm, which we estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIR algorithm: \n",
    "\n",
    "def SIR(m,k,S_0,sigma,mu,M,c,kappa):\n",
    "    \n",
    "    S = np.zeros((M,m+1))\n",
    "    S[:,0], weights = [S_0]*M, [1/M]*M\n",
    "    cut_off, normalizing_constant = M*c, 1.0\n",
    "    mean, sd = (mu-0.5*sigma**2)*k/360, sigma*np.sqrt(k/360)\n",
    "    \n",
    "    S[:,1] = S[:,0] + mean + sd*norm.rvs(0,1,M)\n",
    "    for n in np.add(2,range(m-1)):                                          # n = 2, 3, ..., m\n",
    "        weights /= np.power(np.absolute(np.sum(f(S[:,1:n]),axis=1)),kappa[n-2])\n",
    "        S[:,n] = S[:,n-1] + mean + sd*norm.rvs(0,1,M)\n",
    "        weights *= np.power(np.absolute(np.sum(f(S[:,1:(n+1)]),axis=1)),kappa[n-1])\n",
    "        normalizing_constant *= np.sum(weights) \n",
    "        weights /= np.sum(weights)\n",
    "        ESS = 1/np.sum(weights**2)\n",
    "        if(ESS<cut_off):\n",
    "            S[:,1:(n+1)] = S[np.random.choice(a=range(M),size=M,p=weights),1:(n+1)]\n",
    "            weights = [1/M]*M \n",
    "        \n",
    "    return S[:,1:(m+1)], weights, normalizing_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity check: (is SIR code correct?)\n",
    "* The normalizing constant in SIR is approximating $ \\text{E} |\\sum_{i=1}^{m} f(S_{i}) |^{\\kappa_{m}} $\n",
    "* Let us do a naive Monte Carlo to verify that this is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Monte Carlo to check if SIR is working okay:\n",
    "\n",
    "def naive_MC_SIR(m,k,S_0,sigma,mu,M):\n",
    "    S, f_S = [S_0]*M, np.zeros(M) \n",
    "    mean, sd = (mu-0.5*sigma**2)*k/360, sigma*np.sqrt(k/360)\n",
    "    for i in range(m):                                                   # i = 1, 2, ..., m\n",
    "        S += mean + sd*norm.rvs(0,1,M)\n",
    "        f_S += f(S)\n",
    "    return np.mean(np.power(np.absolute(f_S),kappa[m-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7.8591571836403809, 7.8589780772055207)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, k, M = 30, 30, 10**4\n",
    "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
    "S_0, sigma, mu = np.log(100), 3.0, 0.0\n",
    "c_SIR = 0.5\n",
    "\n",
    "SIR(m,k,S_0,sigma,mu,M,c_SIR,kappa)[2], naive_MC_SIR(m,k,S_0,sigma,mu,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.4172700394946771, 5.3864759268689131)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, k, M = 15, 60, 10**4\n",
    "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
    "S_0, sigma, mu = np.log(100), 2.0, 0.0\n",
    "c_SIR = 0.5\n",
    "\n",
    "SIR(m,k,S_0,sigma,mu,M,c_SIR,kappa)[2], naive_MC_SIR(m,k,S_0,sigma,mu,M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Okay, so SIR seems to be working okay! \n",
    "* This took some time to get correct (unfortunately), but at least it works. \n",
    "* Is the variance of the SIR estimate less than the variance of the naive MC estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMC Sampler:\n",
    "* Again, we do a simplificaton because the model is simple. \n",
    "* What we want to do is to sample from \n",
    "$$ \\tilde{\\pi}_{n} (s_{1:m}) \\propto |\\sum_{i=1}^{m} f(s_{i}) |^{\\tilde{\\kappa}_{n}} p(s_{1:m}) $$\n",
    "for $n = 1, \\ldots, p$ and $ \\kappa_{m} = \\tilde{\\kappa}_{1} < \\ldots < \\tilde{\\kappa}_{p} = 1 $\n",
    "* SIR gives a bunch of particles $\\{ s_{1:m}^{(l)} \\}_{l=1}^{M}$ with weights $\\{ w^{(l)} \\}_{l=1}^{M} $.\n",
    "* Let $x^{(l)} = \\{ s_{1:m}^{(l)} \\}$, $l = 1, \\ldots, M$.\n",
    "\n",
    "So how do we do SMC sampling? \n",
    "* For each particle $x^{(l)}$, we initialize particles $x_{0}^{(l)} = x^{(l)}$ and weights $w_{0}^{(l)} = w^{(l)}$. We do this using the SIR algorithm algorithm. The weights are obtained from the SIR algorithm.\n",
    "* Move $x_{n-1}^{(l)} \\rightarrow x_{n}^{(l)}$ by using kernel $K_{n}(x_{n-1}^{(l)},x_{n})$.\n",
    "* Compute incremental weights:\n",
    "$$ W_{n}^{(l)}(x_{(n-1):n}^{(l)}) = \\frac{ \\tilde{\\pi}_{n}(x_{n}^{(l)}) L_{n-1}(x_{n}^{(l)},x_{n-1}^{(l)})}{ \\tilde{\\pi}_{n-1}(x_{n-1}^{(l)}) K_{n}(x_{n-1}^{(l)},x_{n}^{(l)}) } $$ \n",
    "and then resample if need be.\n",
    "* The choice of backward kernel is:\n",
    "$$ L_{n-1}(x_{n},x_{n-1}) = \\frac{ \\tilde{\\pi}_{n}(x_{n-1}) K_{n}(x_{n-1},x_{n}) }{ \\tilde{\\pi}_{n}(x_{n}) }, $$\n",
    "in which case the incremental weights reduce down to:\n",
    "$$ W_{n}^{(l)}(x_{n-1}^{(l)}) = \\frac{\\tilde{\\pi}_{n}(x_{n-1}^{(l)})}{\\tilde{\\pi}_{n-1}(x_{n-1}^{(l)})} = \\frac{ | \\sum_{i=1}^{m} f(s_{i,n-1}^{(l)}) |^{\\tilde{\\kappa}_{n}} } { | \\sum_{i=1}^{m} f(s_{i,n-1}^{(l)}) |^{\\tilde{\\kappa}_{n-1}} }, \\text{ } n \\geq 1, $$\n",
    "where $ x_{n}^{(l)} = s_{1:m,n}^{(l)}$, $n = 1, 2, \\ldots, p$ and $l = 1, 2, \\ldots, M$.\n",
    "\n",
    "* What is $ \\tilde{\\pi}_{0}(x_{0}^{(l)})$? These are (from the SIR model): \n",
    "$$\\pi_{m}(s^{(l)}_{1:m}) \\propto | \\sum_{i=1}^{m} f(s^{(l)}_{i}) |^{\\kappa_{m}} p(s^{(l)}_{1:m}) = \\tilde{\\pi}_{n} (s^{(l)}_{1:m})$$ \n",
    "as $\\tilde{\\kappa}_{n} = \\kappa_{m} $.\n",
    "\n",
    "The question that remains is how to choose the kernel $K_{n}$?\n",
    "* We want $K_{n}(x_{n-1},\\cdot)$ to be $\\tilde{\\pi}_{n}(\\cdot)$ invariant.\n",
    "* The simplest way to do this is to do a symmetric Metropolis-Hastings random walk.\n",
    "* Therefore, propose $S_{n} \\sim \\text{N}(x_{n-1},\\text{I})$ and accept with probability \n",
    "$$1 \\wedge \\frac{\\tilde{\\pi}_{n}(X_{n})}{\\tilde{\\pi}_{n}(s_{n-1})} = 1 \\wedge \\frac{|\\sum_{i=1}^{m}f(S_{i})|^{\\tilde{\\kappa}_{n}} p(S_{1:m})}{ |\\sum_{i=1}^{m}f(s_{i})|^{\\tilde{\\kappa}_{n}} p(s_{1:m}) } $$\n",
    "\n",
    "A note on the normalizing constant:\n",
    "* $Z_{1}^{SMC} = \\int | \\sum_{i=1}^{m} f(s_{i}) |^{\\tilde{\\kappa}_{1}} p(s_{1:m}) \\text{d} s_{1:m} = Z_{m}^{SIR}$ as $\\tilde{\\kappa}_{1} = \\kappa_{m}$.\n",
    "* $ Z_{p}^{SMC} = \\int | \\sum_{i=1}^{m} f(s_{i}) | p(s_{1:m}) \\text{d} s_{1:m} $ is the normalizing constant in the SMC algorithm, and is precisely what we want to estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<numpy.lib.function_base.vectorize at 0xa11aeb8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SMC samplers: \n",
    "# We work on a log scale for densities because they tend to be tiny.\n",
    "# p(s_{1:m}): \n",
    "# This is the density of the path of a Brownian Motion starting at S_0\n",
    "# ('path' does not contain S_0)\n",
    "# We consider the log of the density\n",
    "\n",
    "def log_path_density(sigma,mu,s_0,path):\n",
    "    path = np.append(s_0,path)\n",
    "    N = np.size(path)\n",
    "    mean = path[0:(N-1)] + (mu-0.5*sigma**2)*k/360.0\n",
    "    sd = sigma*np.sqrt(k/360.0)\n",
    "    return np.sum(np.log(norm.pdf(path[1:N],loc=mean,scale=sd)))\n",
    "np.vectorize(log_path_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symmetric random walk Metropolis-Hastings kernel:\n",
    "# (No new array created)\n",
    "def propagate(original,proposed,n):\n",
    "    proposed = norm.rvs(loc=original,scale=1.0) \n",
    "    a = np.power(np.absolute(np.sum(f(proposed))/np.sum(f(original))),kappa_tilde[n])\n",
    "    b = np.exp(log_path_density(sigma,mu,S_0,proposed)-log_path_density(sigma,mu,S_0,original))\n",
    "    MH_ratio = a*b\n",
    "    acceptance_probability = 1.0\n",
    "    if(MH_ratio<1.0):\n",
    "        acceptance_probability = MH_ratio\n",
    "    u = uniform.rvs(size=1)\n",
    "    if(u<acceptance_probability):\n",
    "        original = proposed\n",
    "    return original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the SMC function:\n",
    "\n",
    "def SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde):\n",
    "    \n",
    "    # We first run SIR with M particles:\n",
    "    S, weights, SIR_normalizing_constant = SIR(m,k,S_0,sigma,mu,M,c_SIR,kappa)\n",
    "    \n",
    "    # Next define some necessary things:\n",
    "    cut_off, normalizing_constant = c_SMC*M, SIR_normalizing_constant\n",
    "    incremental_weights, proposed = np.zeros(M), np.zeros(M)\n",
    "    p = np.size(kappa_tilde)-1\n",
    "\n",
    "    for n in np.add(1,range(p)):                                              # n = 1, 2, ..., p\n",
    "        # Propagating and computing incremental weights:\n",
    "        # (hopefully we shall later be able to do without the loop over i)\n",
    "        for i in range(M):                                                              \n",
    "            incremental_weights[i] = np.power(np.absolute(np.sum(f(S[i,:]))),kappa_tilde[n]-kappa_tilde[n-1])\n",
    "            S[i,:] = propagate(S[i,:],proposed,n)\n",
    "        weights *= incremental_weights\n",
    "        normalizing_constant *= np.sum(weights) \n",
    "        weights /= np.sum(weights)\n",
    "        ESS = 1/np.sum(weights**2)\n",
    "        if(ESS<cut_off):\n",
    "            S = S[np.random.choice(a=range(M),size=M,p=weights),:]\n",
    "            weights = [1/M]*M \n",
    "        \n",
    "    return normalizing_constant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Monte Carlo: \n",
    "* As a comparison, we can also use naive Monte Carlo to estimate the price. \n",
    "* We simulate a bunch of $M$ paths of the process independently of each other and then estimate the option price as \n",
    "$$ \\frac{1}{M} \\sum_{l=1}^{M} \\left [ \\sum_{i=1}^{m} f(S_{i}^{(l)}) \\right ] $$\n",
    "where we again use the simplification $S_{1:m}$ for $S_{t_{1}:t_{m}}$ nd where $S_{1:m}^{(l)}$ denotes the $l$-th path.\n",
    "* However, since the SMC algorithm is essentially approximating $ \\text{E} | \\sum_{i=1}^{m} f(S_{i}) | $, let us also use naive Monte Carlo to estimate the same quantity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Monte Carlo\n",
    "\n",
    "def naive_MC(m,k,S_0,sigma,mu,M):\n",
    "    m = np.int(m)\n",
    "    price = np.zeros(M)\n",
    "    S = [S_0]*M \n",
    "    mean = [(mu - 0.5*sigma**2)*k/360.0]*M\n",
    "    sd = sigma*np.sqrt(k/360.0)\n",
    "    for i in np.add(1,range(m)):                                                    # i = 1, 2, ..., m\n",
    "        S = np.add(S,mean+sd*norm.rvs(0,1,M))\n",
    "        price += f(S)\n",
    "    return np.mean(np.absolute(price)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Another sanity check (is the code for the SMC sampler correct?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kappa = [ 0.          0.02631579  0.05263158  0.07894737  0.10526316  0.13157895\n",
      "  0.15789474  0.18421053  0.21052632  0.23684211  0.26315789  0.28947368\n",
      "  0.31578947  0.34210526  0.36842105  0.39473684  0.42105263  0.44736842\n",
      "  0.47368421  0.5       ]\n",
      "kappa_tilde = [ 0.5         0.53571429  0.57142857  0.60714286  0.64285714  0.67857143\n",
      "  0.71428571  0.75        0.78571429  0.82142857  0.85714286  0.89285714\n",
      "  0.92857143  0.96428571  1.        ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(38.433763162998709, 38.294483153781805)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m, k, M, p = 20, 30, 10**4, 15\n",
    "kappa = np.linspace(start=0.0,stop=0.5,num=m)\n",
    "kappa_tilde = np.linspace(start=0.5,stop=1.0,num=p)\n",
    "S_0, sigma, mu = np.log(100), 3.0, 0.0\n",
    "c_SIR, c_SMC = 0.5, 0.5\n",
    "\n",
    "print \"kappa =\", kappa\n",
    "print \"kappa_tilde =\", kappa_tilde\n",
    "\n",
    "SMC(m,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde), naive_MC(m,k,S_0,sigma,mu,M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6AAAAE4CAYAAABFb562AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBlBJkiRJUrP4/6y1xXNcdrZtAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xa1a3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m, k, M, p = 20.0, 30, 10**3, 15\n",
    "m = np.int(m)\n",
    "\n",
    "survival_probability_naive_MC = np.zeros(m)\n",
    "survival_probability_SMC = np.zeros(m)\n",
    "naive_MC_price = np.zeros(m)\n",
    "SMC_sampler_price = np.zeros(m)\n",
    "\n",
    "kappa_tilde = np.linspace(start=0.5,stop=1.0,num=p)\n",
    "\n",
    "for t in np.add(1,range(m)):                              # t = 1, 2, ..., m\n",
    "    kappa = np.linspace(start=0.0,stop=0.5,num=t)\n",
    "    naive_MC_price[t-1] = naive_MC(t,k,S_0,sigma,mu,M)\n",
    "    SMC_sampler_price[t-1] = SMC(t,k,S_0,sigma,mu,M,c_SIR,c_SMC,kappa,kappa_tilde)\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(np.add(1,range(m)),naive_MC_price,\"r*\",np.add(1,range(m)),SMC_sampler_price,\"b-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The code is working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic comparison study: \n",
    "* We now look at the standard errors for the naive MC estimate and the SMC estimate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMC estimate = 38.452\n",
      "MC estimate = 38.462\n",
      "SMC standard deviation = 0.929\n",
      "MC standard deviation = 0.238\n",
      "SMC average run time = 3.094 seconds\n",
      "MC average run time = 0.013 seconds\n",
      "Overall run time for 50 repetitions = 2.589 minutes\n"
     ]
    }
   ],
   "source": [
    "rep = 50\n",
    "SMC_estimate = np.zeros(rep)\n",
    "MC_estimate = np.zeros(rep)\n",
    "\n",
    "overall_start_time = time.clock()\n",
    "start_time = time.clock()\n",
    "for r in range(rep):\n",
    "    SMC_estimate[r] = SMC(m,k,S_0,sigma,mu,10**3,c_SIR,c_SMC,kappa,kappa_tilde)\n",
    "average_SMC_run_time = (time.clock() - start_time)/rep\n",
    "start_time = time.clock()\n",
    "for r in range(rep):\n",
    "    MC_estimate[r] = naive_MC(m,k,S_0,sigma,mu,10**4) \n",
    "average_MC_run_time = (time.clock() - start_time)/rep\n",
    "overall_run_time = time.clock() - overall_start_time\n",
    "\n",
    "print \"SMC estimate =\", round(np.mean(SMC_estimate),3)\n",
    "print \"MC estimate =\", round(np.mean(MC_estimate),3)\n",
    "print \"SMC standard deviation =\", round(np.std(SMC_estimate),3)\n",
    "print \"MC standard deviation =\", round(np.std(MC_estimate),3)\n",
    "print \"SMC average run time =\", round(average_SMC_run_time,3), \"seconds\"\n",
    "print \"MC average run time =\", round(average_MC_run_time,3), \"seconds\"\n",
    "print \"Overall run time for\", rep, \"repetitions =\", round(overall_run_time/60,3), \"minutes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
